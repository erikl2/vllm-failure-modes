================================================================================
OPTION C EXPERIMENTAL RESULTS REPORT
Generated: 2025-12-11 06:00:41
================================================================================

RELIABILITY MATRIX
--------------------------------------------------------------------------------
GPU Mem   C=1         C=2         C=4         C=8         C=16        
--------------------------------------------------------------------------------
0.70      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    
0.75      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    
0.80      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    
0.85      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    
0.90      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    
0.95      100% (3)    100% (3)    100% (3)    100% (3)    100% (3)    

STABILITY FRONTIER (Minimum GPU memory for 100% reliability)
--------------------------------------------------------------------------------
  Concurrency  1: GPU memory >= 0.70
  Concurrency  2: GPU memory >= 0.70
  Concurrency  4: GPU memory >= 0.70
  Concurrency  8: GPU memory >= 0.70
  Concurrency 16: GPU memory >= 0.70

KEY FINDINGS FOR PAPER
--------------------------------------------------------------------------------
  • GPU memory >= 0.70 is stable at ALL concurrency levels

SUGGESTED PAPER TEXT (corrected from original)
--------------------------------------------------------------------------------

The stability frontier shows that minimum safe GPU memory utilization
scales with concurrency level. For our test configuration (Llama-3.1-8B
on A100 40GB), we observe:

[INSERT SPECIFIC VALUES FROM YOUR RESULTS]

Importantly, we observed run-to-run variance in crash behavior at
boundary configurations, with identical settings sometimes succeeding
and sometimes failing. This variance reflects non-deterministic factors
including GPU memory allocator state, request scheduling timing, and
prompt ordering effects. Our use of shuffled prompts (randomized per
seed) contributes to this variance by creating different KV cache growth
patterns across runs.

This finding underscores that single-run validation is insufficient for
production readiness testing—multiple runs with varied conditions are
essential to characterize true reliability.


================================================================================