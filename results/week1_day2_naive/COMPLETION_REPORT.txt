================================================================================
WEEK 1 DAY 2 COMPLETION REPORT
================================================================================
Date: 2025-10-29 23:30 UTC
Task: Naive Baseline Benchmark
Model: meta-llama/Llama-3.1-8B-Instruct
GPU: NVIDIA GH200 480GB

FILES GENERATED:
================================================================================
- workload.json (144KB) - 100 prompts, 70 short (50-62 tokens), 30 long (500-509 tokens)
- results_naive.csv - Raw benchmark results for 3 seeds × 100 prompts
- results_naive_meta.json - Experiment metadata (GPU, CUDA, PyTorch versions)
- summary_naive.txt - Formatted statistics table

BENCHMARK RESULTS SUMMARY:
================================================================================

Metric                                        Mean ± Std
--------------------------------------------------------------------------------
P50 Latency                              3534.5 ± 6.3 ms
P95 Latency                              3557.9 ± 8.1 ms
P99 Latency                             3572.1 ± 11.2 ms
Throughput                              36.2 ± 0.1 tok/s
Peak Memory                                15.1 ± 0.0 GB
--------------------------------------------------------------------------------

Total time per seed:                      353.8 ± 0.6 s (~5.9 minutes)
Seeds tested:                             42, 43, 44
Total inferences:                         300 (3 seeds × 100 prompts)
Max tokens generated per prompt:          128 tokens

EXPERIMENT DETAILS:
================================================================================
- Workload: Poisson arrivals (λ=10 req/s), realistic ML/AI prompts
- Inference: Sequential (no batching), one prompt at a time
- Model precision: torch.float16
- Device: auto (automatically placed on GPU)
- Reproducibility: Random seeds set for torch, numpy, random

GPU TIME USED:
================================================================================
Total GPU time: ~18 minutes
Estimated cost: ~$0.30 @ $1.00/hour for GH200 480GB

KEY OBSERVATIONS:
================================================================================
1. Very consistent performance across seeds (low std deviation)
2. P99 latency only ~38ms higher than P50 (very stable)
3. Throughput: 36.2 tokens/second for sequential processing
4. Memory usage: 15.1 GB (plenty of headroom on 480GB GPU)
5. Average inference time: ~3.5 seconds per prompt

TECHNICAL NOTES:
================================================================================
- Initially PyTorch installed without CUDA support (CPU-only)
- Fixed by reinstalling: pip install torch --index-url https://download.pytorch.org/whl/cu124
- HuggingFace authentication required for Llama-3.1-8B-Instruct (gated model)
- Auto-terminate script successfully set up with 210-minute timeout

STATUS: ✅ COMPLETE
================================================================================

NEXT STEPS:
- Week 1 Day 3: vLLM comparison benchmark
- Compare naive baseline vs. continuous batching performance
- Analyze throughput improvements and latency trade-offs

================================================================================
End of Report
================================================================================
